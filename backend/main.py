"""
IBK ì¹´ë“œ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ API - Main Application
ì‹¤ì œ ML ëª¨ë¸ ë¡œë”© ë° ì „ì²´ ì‹œìŠ¤í…œ í†µí•©

Copyright (c) 2024 (ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜
"""

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import logging
import os
from pathlib import Path

# Routers
from api.routes import predict, dashboard, campaigns, customers

# Services
from services.db import init_db, check_db_connection
from services.cache import is_redis_available, get_cache_stats
from services.scheduler import start_scheduler, stop_scheduler
from models.churn_predictor import ChurnPredictor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
ml_model: ChurnPredictor = None


def load_ml_model():
    """ML ëª¨ë¸ ë¡œë”©"""
    global ml_model
    
    try:
        model_path = os.getenv("MODEL_PATH", "../ml/models/churn_model_latest.pkl")
        
        # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
        if not os.path.isabs(model_path):
            model_path = os.path.join(os.path.dirname(__file__), model_path)
        
        if not os.path.exists(model_path):
            logger.warning(f"âš ï¸ Model file not found: {model_path}")
            logger.warning("   Please run: train_model.bat")
            logger.warning("   Starting with mock predictions...")
            return None
        
        logger.info(f"ğŸ“¦ Loading model from: {model_path}")
        ml_model = ChurnPredictor.load_from_file(model_path)
        logger.info("âœ… ML model loaded successfully!")
        
        return ml_model
        
    except Exception as e:
        logger.error(f"âŒ Failed to load ML model: {e}", exc_info=True)
        logger.warning("   Starting with mock predictions...")
        return None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ìƒëª…ì£¼ê¸° ê´€ë¦¬"""
    logger.info("="*60)
    logger.info("ğŸš€ Starting IBK Churn Prevention API")
    logger.info("   (ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜")
    logger.info("="*60)
    
    # 1. ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
    logger.info("\n[1/5] Initializing database...")
    try:
        init_db()
        if check_db_connection():
            logger.info("   âœ… Database connected")
        else:
            logger.warning("   âš ï¸ Database not available (using mock data)")
    except Exception as e:
        logger.warning(f"   âš ï¸ Database init failed: {e}")
    
    # 2. Redis ìºì‹± í™•ì¸
    logger.info("\n[2/5] Checking Redis cache...")
    if is_redis_available():
        stats = get_cache_stats()
        logger.info(f"   âœ… Redis connected - {stats.get('total_keys', 0)} keys")
    else:
        logger.warning("   âš ï¸ Redis not available (caching disabled)")
    
    # 3. ML ëª¨ë¸ ë¡œë”©
    logger.info("\n[3/5] Loading ML models...")
    model = load_ml_model()
    if model:
        logger.info("   âœ… ML model ready")
        # ëª¨ë¸ì„ app.stateì— ì €ì¥
        app.state.ml_model = model
    else:
        logger.warning("   âš ï¸ ML model not loaded")
        app.state.ml_model = None
    
    # 4. ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ìë™ ë¦¬í¬íŠ¸)
    logger.info("\n[4/5] Starting scheduler...")
    if os.getenv("ENABLE_SCHEDULER", "true").lower() == "true":
        try:
            start_scheduler()
            logger.info("   âœ… Scheduler started (daily & weekly reports)")
        except Exception as e:
            logger.warning(f"   âš ï¸ Scheduler failed: {e}")
    else:
        logger.info("   â¸ï¸ Scheduler disabled (set ENABLE_SCHEDULER=true to enable)")
    
    # 5. ì‹œìŠ¤í…œ ì •ë³´
    logger.info("\n[5/5] System information:")
    logger.info(f"   ğŸ“ API Docs: http://localhost:8000/docs")
    logger.info(f"   ğŸ” Health: http://localhost:8000/health")
    logger.info(f"   ğŸ—„ï¸ Database: {'Connected' if check_db_connection() else 'Disconnected'}")
    logger.info(f"   ğŸ’¾ Redis: {'Connected' if is_redis_available() else 'Disconnected'}")
    logger.info(f"   ğŸ¤– ML Model: {'Loaded' if model else 'Mock Mode'}")
    
    logger.info("\n" + "="*60)
    logger.info("âœ… API READY!")
    logger.info("="*60 + "\n")
    
    yield
    
    # Shutdown
    logger.info("\nğŸ‘‹ Shutting down...")
    try:
        stop_scheduler()
    except:
        pass
    logger.info("Goodbye!")


# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="IBK ì¹´ë“œ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ API",
    description="""
    AI ê¸°ë°˜ ì¹´ë“œ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ë° ë°©ì§€ ì‹œìŠ¤í…œ
    
    **ì£¼ìš” ê¸°ëŠ¥:**
    - ì‹¤ì‹œê°„ ì´íƒˆ ì˜ˆì¸¡ (XGBoost, LightGBM, Random Forest ì•™ìƒë¸”)
    - SHAP ê¸°ë°˜ ì„¤ëª… ê°€ëŠ¥í•œ AI
    - ìƒì• ì£¼ê¸°ë³„ ë§ì¶¤ ë¶„ì„
    - ìë™ ì¼ì¼/ì£¼ê°„ ë¦¬í¬íŠ¸
    - ìº í˜ì¸ íš¨ê³¼ ì¸¡ì •
    
    **ê°œë°œ:** (ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜
    """,
    version="2.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ë¼ìš°í„° í¬í•¨
app.include_router(predict.router, prefix="/api", tags=["Prediction"])
app.include_router(dashboard.router, prefix="/api", tags=["Dashboard"])
app.include_router(campaigns.router, prefix="/api", tags=["Campaigns"])
app.include_router(customers.router, prefix="/api", tags=["Customers"])


@app.get("/", tags=["Root"])
async def root():
    """API ë£¨íŠ¸"""
    return {
        "message": "IBK ì¹´ë“œ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ API",
        "company": "(ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜",
        "version": "2.0.0",
        "status": "healthy",
        "docs": "/docs",
        "health": "/health"
    }


@app.get("/health", tags=["Health"])
async def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    db_connected = check_db_connection()
    redis_connected = is_redis_available()
    model_loaded = hasattr(app.state, 'ml_model') and app.state.ml_model is not None
    
    return {
        "status": "healthy",
        "api": "operational",
        "database": "connected" if db_connected else "disconnected",
        "cache": "connected" if redis_connected else "disconnected",
        "model": "loaded" if model_loaded else "not_loaded",
        "company": "(ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜"
    }


@app.get("/api/system/info", tags=["System"])
async def system_info():
    """ì‹œìŠ¤í…œ ì •ë³´"""
    return {
        "company": "(ì£¼)ë²”ì˜¨ëˆ„ë¦¬ ì´ë…¸ë² ì´ì…˜",
        "system": "IBK ì¹´ë“œ ê³ ê° ì´íƒˆ ì˜ˆì¸¡ AI",
        "version": "2.0.0",
        "ml_model": {
            "loaded": hasattr(app.state, 'ml_model') and app.state.ml_model is not None,
            "type": "XGBoost + LightGBM + Random Forest Ensemble",
            "features": "100+ engineered features",
            "explainability": "SHAP-based"
        },
        "database": {
            "connected": check_db_connection(),
            "type": "PostgreSQL / SQLite"
        },
        "cache": get_cache_stats(),
        "scheduler": {
            "enabled": os.getenv("ENABLE_SCHEDULER", "true").lower() == "true",
            "jobs": ["Daily Report (08:00)", "Weekly Summary (Mon 09:00)"]
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
